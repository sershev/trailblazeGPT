{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61f84f1c-1b13-4a53-af09-6c81a49bbfe4",
      "metadata": {
        "id": "61f84f1c-1b13-4a53-af09-6c81a49bbfe4"
      },
      "source": [
        "# Week 2: Turning Words into Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b923e5-bd9b-4c24-9324-5d7bf8999875",
      "metadata": {
        "id": "48b923e5-bd9b-4c24-9324-5d7bf8999875"
      },
      "source": [
        "In this notebook, we will explore the process of converting text into tokens, a fundamental step in NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ca97c1-8bf4-4270-8963-97f31824d773",
      "metadata": {
        "id": "b9ca97c1-8bf4-4270-8963-97f31824d773"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8655f2-ae2d-49de-8add-d5bd2af993dc",
      "metadata": {
        "id": "4e8655f2-ae2d-49de-8add-d5bd2af993dc"
      },
      "source": [
        "We will begin by importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9560f8d8-0e98-485e-9d97-88f3838e1b9a",
      "metadata": {
        "id": "9560f8d8-0e98-485e-9d97-88f3838e1b9a"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2e63d757-efa7-4008-9d53-6ad0aed6beb5",
      "metadata": {
        "id": "2e63d757-efa7-4008-9d53-6ad0aed6beb5"
      },
      "outputs": [],
      "source": [
        "# Import file checking whether TODO has been removed\n",
        "#from helpers.check_todo import check_implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ea13119-c573-47c5-876b-e9f866b88eb7",
      "metadata": {
        "id": "9ea13119-c573-47c5-876b-e9f866b88eb7"
      },
      "outputs": [],
      "source": [
        "# Create the src directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('src', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa88576-c301-4f54-a63c-d6b2172419e7",
      "metadata": {
        "id": "dfa88576-c301-4f54-a63c-d6b2172419e7"
      },
      "source": [
        "## 1. Running Simple Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2145f93-25be-4d74-9a82-5fa3e11765e1",
      "metadata": {
        "id": "d2145f93-25be-4d74-9a82-5fa3e11765e1"
      },
      "source": [
        "This section demonstrates a basic approach to tokenization using Python's built-in libraries and PyTorch. We will implement a basic tokenization function. This function will split the text into individual tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9009535b-4245-4033-9fe0-b431d7885cef",
      "metadata": {
        "id": "9009535b-4245-4033-9fe0-b431d7885cef"
      },
      "outputs": [],
      "source": [
        "sample_text = \"Hello, how are you doing today?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3518217b-5339-4983-89c7-3e8518f5984e",
      "metadata": {
        "id": "3518217b-5339-4983-89c7-3e8518f5984e"
      },
      "outputs": [],
      "source": [
        "code_text = \"\"\"\n",
        "def calculate_llm_perplexity(model, text, max_length=1024):\n",
        "    tokens = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
        "    input_ids = torch.tensor([tokens]).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    loss = outputs.loss\n",
        "    return math.exp(loss.item())\n",
        "\n",
        "# Example usage\n",
        "perplexity = calculate_llm_perplexity(gpt2_model, \"Hello, world!\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5eb0ce8b-7b50-4b59-9882-a1c0003c2ada",
      "metadata": {
        "id": "5eb0ce8b-7b50-4b59-9882-a1c0003c2ada"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    # Hint: Use regex to split the text into words and punctuation\n",
        "    split = map(str.strip, re.split(\"(\\s+|,+|\\.+|\\!+|\\?+|=+|\\(+|\\)+|\\[+|\\]+)\", text))\n",
        "    tokens = [s for s in split if s]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f66d39-42fc-43d2-ab00-10e9a2cb4148",
      "metadata": {
        "id": "70f66d39-42fc-43d2-ab00-10e9a2cb4148"
      },
      "source": [
        "Our time to test whether you have reviewed 'TODO' in the first function we implement together. Remove 'TODO' once you're done implementing and no error message will appear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7c4f3c56-03dc-413b-85c8-e9aca0e33f29",
      "metadata": {
        "id": "7c4f3c56-03dc-413b-85c8-e9aca0e33f29"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # check_implementation(tokenize)\n",
        "    pass\n",
        "except NotImplementedError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "286235de-79b4-4703-a4d6-95aef3c006af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "286235de-79b4-4703-a4d6-95aef3c006af",
        "outputId": "f3851f6e-0360-41b8-9ed4-814fd09bdac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized text: ['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokenized text:\", tokenize(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "63bf3e36-e525-4b4f-a9e0-202ef052d52f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63bf3e36-e525-4b4f-a9e0-202ef052d52f",
        "outputId": "8f0f1da8-cd40-40c4-e03e-0c6c2951a98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized code: ['def', 'calculate_llm_perplexity', '(', 'model', ',', 'text', ',', 'max_length', '=', '1024', ')', ':', 'tokens', '=', 'tokenizer', '.', 'encode', '(', 'text', ',', 'max_length', '=', 'max_length', ',', 'truncation', '=', 'True', ')', 'input_ids', '=', 'torch', '.', 'tensor', '(', '[', 'tokens', ']', ')', '.', 'to', '(', 'device', ')', 'with', 'torch', '.', 'no_grad', '(', ')', ':', 'outputs', '=', 'model', '(', 'input_ids', ',', 'labels', '=', 'input_ids', ')', 'loss', '=', 'outputs', '.', 'loss', 'return', 'math', '.', 'exp', '(', 'loss', '.', 'item', '(', '))', '#', 'Example', 'usage', 'perplexity', '=', 'calculate_llm_perplexity', '(', 'gpt2_model', ',', '\"Hello', ',', 'world', '!', '\"', ')', 'print', '(', 'f\"Perplexity:', '{perplexity:', '.', '2f}\"', ')']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokenized code:\", tokenize(code_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148c43a7-3e71-487c-92bc-4a3f5376f919",
      "metadata": {
        "id": "148c43a7-3e71-487c-92bc-4a3f5376f919"
      },
      "source": [
        "## 2. Creating a Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f703890-f0b4-4d75-a1c4-37ea1ec4f12f",
      "metadata": {
        "id": "8f703890-f0b4-4d75-a1c4-37ea1ec4f12f"
      },
      "source": [
        "In this section we will create a function that takes a list of texts as input and returns a dictionary. In it each key is a unique word (or token) from the texts and its corresponding value is a unique index. The function should also reserve a special token <UNK> with index 0 to represent unknown words that may appear in future texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a448895f-faf6-496f-89d7-cb225e9c0734",
      "metadata": {
        "id": "a448895f-faf6-496f-89d7-cb225e9c0734"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(texts):\n",
        "\n",
        "    # Do not forget to reserve a slot for unknown tokens\n",
        "    tokens = [token for text in texts for token in tokenize(text)]\n",
        "    unique_tokens = set(tokens)\n",
        "    vocab = {token: id for id, token in enumerate(unique_tokens)}\n",
        "    vocab[\"[UNK]\"] = len(vocab)\n",
        "\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f3191793-955f-4e77-9c18-179e602157d6",
      "metadata": {
        "id": "f3191793-955f-4e77-9c18-179e602157d6"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # check_implementation(build_vocabulary)\n",
        "    pass\n",
        "except NotImplementedError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ca3c4f37-1dc6-48a2-bd9a-064c579587f1",
      "metadata": {
        "id": "ca3c4f37-1dc6-48a2-bd9a-064c579587f1"
      },
      "outputs": [],
      "source": [
        "# TODO: Use your examples for a sample dataset\n",
        "# We won't be checking whether you have removed TODO here\n",
        "# But using your own sentences is encouraged!\n",
        "\n",
        "sample_dataset = [\n",
        "    \"42 is the Ultimate answer for Life, the Universe, and Everything.\",\n",
        "    \"Hello, world of LLM Trailblazers! This is another example.\",\n",
        "    \"What is the weather like today in Munich?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3c2e8bf6-d345-4607-aae9-592f06c5b7d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c2e8bf6-d345-4607-aae9-592f06c5b7d1",
        "outputId": "388db409-774e-4a7e-fdee-de0cee75ae38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'Everything': 0, 'is': 1, 'the': 2, 'for': 3, '42': 4, 'Trailblazers': 5, 'another': 6, 'Munich': 7, '.': 8, 'What': 9, 'today': 10, '!': 11, 'answer': 12, 'This': 13, 'weather': 14, 'Life': 15, '?': 16, 'like': 17, 'in': 18, 'Hello': 19, 'Ultimate': 20, 'world': 21, ',': 22, 'of': 23, 'LLM': 24, 'example': 25, 'and': 26, 'Universe': 27, '[UNK]': 28}\n"
          ]
        }
      ],
      "source": [
        "vocab = build_vocabulary(sample_dataset)\n",
        "print(\"Vocabulary:\", vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac055e16-0b54-40d9-9b8d-3bb05518c1ef",
      "metadata": {
        "id": "ac055e16-0b54-40d9-9b8d-3bb05518c1ef"
      },
      "source": [
        "## 3. Implementing a Custom Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c078f7-0b94-416c-9333-60c7b7639cea",
      "metadata": {
        "id": "66c078f7-0b94-416c-9333-60c7b7639cea"
      },
      "source": [
        "We have a lot of text data, but it's all different lengths. We need to make it work for our model. To do this, we'll create two special helpers:\n",
        "\n",
        "1. A `Dataset` class: This will help us prepare our text data for our model. We'll break down the text into smaller pieces and convert it into a format our model can understand.\n",
        "2. A `DataLoader` class: This will help us feed our prepared data to our model in batches. We'll sort the batches by length, add padding to make them all the same size, and create a mask to ignore the extra padding.\n",
        "\n",
        "By using these two helpers, we'll be able to get our data in order and make it easy for our model to work with. This will make our training process smoother and more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "2f8ab068-e39a-4fa1-9563-35a52a34cd23",
      "metadata": {
        "id": "2f8ab068-e39a-4fa1-9563-35a52a34cd23"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, vocab):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with texts and vocabulary.\n",
        "\n",
        "        :param texts: A list of text samples.\n",
        "        :param vocab: A dictionary representing the vocabulary, where keys are tokens and values are their corresponding IDs.\n",
        "        \"\"\"\n",
        "        self.texts = texts\n",
        "        self.vocab = vocab\n",
        "        self.unk_id = self.vocab[\"[UNK]\"]\n",
        "        assert len(self.vocab) == (self.unk_id + 1)\n",
        "        self.stride=1\n",
        "        self.sample_len=4\n",
        "        self.__prepare__()\n",
        "\n",
        "    def __prepare__(self):\n",
        "        for text in self.texts:\n",
        "          token_ids = [self.vocab.get(token, self.unk_id) for token in tokenize(text)]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        tokens = [self.vocab.get(token, self.unk_id) for token in tokenize(self.texts[idx])]\n",
        "        return tokens[:-1], tokens[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "e6060a82-7c14-4038-95e1-156f4a27d6ce",
      "metadata": {
        "id": "e6060a82-7c14-4038-95e1-156f4a27d6ce"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    #check_implementation(TextDataset)\n",
        "    pass\n",
        "except NotImplementedError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "17206567-8921-415a-a495-36524692cbd5",
      "metadata": {
        "id": "17206567-8921-415a-a495-36524692cbd5"
      },
      "outputs": [],
      "source": [
        "# Create a dataset instance\n",
        "dataset = TextDataset(sample_dataset, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "a91ce2dc-f5cf-43bb-bd8f-a8144e406cc1",
      "metadata": {
        "id": "a91ce2dc-f5cf-43bb-bd8f-a8144e406cc1"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "simple_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "6813df26-8873-4452-900f-95ace96f6872",
      "metadata": {
        "id": "6813df26-8873-4452-900f-95ace96f6872",
        "outputId": "ee334f09-1995-44a1-9750-9327375d7af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "each element in list of batch should be of equal size",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-0f0cdab7c3ae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display a batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# It may be accessed twice, so we use a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
          ]
        }
      ],
      "source": [
        "# Display a batch of data\n",
        "for batch in simple_dataloader:\n",
        "    print(\"Batch shape:\", batch.shape)\n",
        "    print(\"Sample batch:\", batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "824291b2-3280-4682-beaf-a5866a0067b9",
      "metadata": {
        "id": "824291b2-3280-4682-beaf-a5866a0067b9",
        "outputId": "e9185fc5-50cc-41d1-bdff-ab54bb19abbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to iterate through the dataloader:\n",
            "Processed batch: tensor([28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(\"Attempting to iterate through the dataloader:\")\n",
        "try:\n",
        "    for batch in simple_dataloader:\n",
        "        print(\"Processed batch:\", batch)\n",
        "        break\n",
        "except RuntimeError as e:\n",
        "    print(f\"Caught an error: {e}\")\n",
        "    print(\"\\nThis error occurs because we're trying to batch sequences of different lengths.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03413975-cc74-40ca-9ab6-644b7610caa8",
      "metadata": {
        "id": "03413975-cc74-40ca-9ab6-644b7610caa8"
      },
      "source": [
        "Now, let's implement a custom collate_fn to handle variable-length sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "195014b6-4b41-4817-b11d-61daaab23d11",
      "metadata": {
        "id": "195014b6-4b41-4817-b11d-61daaab23d11"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Separate the input sequences and targets\n",
        "    sequences, targets = zip(*batch)\n",
        "\n",
        "    # Pad the sequences\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad the targets if they are sequences, otherwise just stack them\n",
        "    if isinstance(targets[0], torch.Tensor) and targets[0].dim() > 0:\n",
        "        padded_targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "    else:\n",
        "        padded_targets = torch.stack(targets)\n",
        "\n",
        "    return padded_sequences, padded_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "543f5e3a-6c54-4c1a-b641-5f11f695d6ee",
      "metadata": {
        "id": "543f5e3a-6c54-4c1a-b641-5f11f695d6ee"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "ad045f95-d5e9-4868-8645-c35831c5ed08",
      "metadata": {
        "id": "ad045f95-d5e9-4868-8645-c35831c5ed08",
        "outputId": "ee7947ea-5caa-40d5-e78c-9ead609d5193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterating through the dataloader with custom collate_fn:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-50bdaba60bdf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iterating through the dataloader with custom collate_fn:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processed batch shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mask shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample batch:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-5e5c0739db8b>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Separate the input sequences and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Pad the sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ],
      "source": [
        "print(\"Iterating through the dataloader with custom collate_fn:\")\n",
        "for batch, mask in dataloader:\n",
        "    print(\"Processed batch shape:\", batch.shape)\n",
        "    print(\"Mask shape:\", mask.shape)\n",
        "    print(\"Sample batch:\")\n",
        "    print(batch)\n",
        "    print(\"Sample mask:\")\n",
        "    print(mask)\n",
        "    break\n",
        "\n",
        "# TODO: Experiment with setting DataLoader with shuffle=False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e94cd25c-9415-407f-a010-b39478526306",
      "metadata": {
        "id": "e94cd25c-9415-407f-a010-b39478526306"
      },
      "source": [
        "The TextProcessor now successfully handles variable-length sequences!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776f9e7b-ddfb-4ec5-87aa-d28bf401179e",
      "metadata": {
        "id": "776f9e7b-ddfb-4ec5-87aa-d28bf401179e"
      },
      "source": [
        "## 4. Putting It All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c8607e-fbaa-44a8-9767-77a718b9c98a",
      "metadata": {
        "id": "74c8607e-fbaa-44a8-9767-77a718b9c98a"
      },
      "source": [
        "Time to combine tokenization, vocabulary creation and data preparation in batches. That's where our `TextProcessor` will help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97cf6f66-3878-43ce-b02c-8977ac07b813",
      "metadata": {
        "id": "97cf6f66-3878-43ce-b02c-8977ac07b813"
      },
      "outputs": [],
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.vocab = None\n",
        "\n",
        "    def tokenize(self, text):\n",
        "\n",
        "        # TODO: Implement tokenization\n",
        "\n",
        "        pass\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "\n",
        "        # TODO: Build vocabulary from a list of texts\n",
        "\n",
        "        pass\n",
        "\n",
        "    def create_dataloader(self, texts, batch_size):\n",
        "\n",
        "        # TODO: Create a DataLoader with TextDataset from a list of texts\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e61a1e65-2a3d-4e4c-93e4-17c3c8cf900d",
      "metadata": {
        "id": "e61a1e65-2a3d-4e4c-93e4-17c3c8cf900d"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    check_implementation(TextProcessor)\n",
        "except NotImplementedError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1215abd5-3e57-48ba-a4b3-071b34709563",
      "metadata": {
        "id": "1215abd5-3e57-48ba-a4b3-071b34709563"
      },
      "outputs": [],
      "source": [
        "# Test the TextProcessor\n",
        "processor = TextProcessor()\n",
        "processor.build_vocab(sample_dataset)\n",
        "dataloader = processor.create_dataloader(sample_dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120a7934-9b3a-4193-85e1-c00ff9d0a433",
      "metadata": {
        "id": "120a7934-9b3a-4193-85e1-c00ff9d0a433"
      },
      "outputs": [],
      "source": [
        "for batch in dataloader:\n",
        "    print(\"Processed batch:\", batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6df659c-84cb-4c39-b0fb-1c3e77f61917",
      "metadata": {
        "id": "b6df659c-84cb-4c39-b0fb-1c3e77f61917"
      },
      "source": [
        "#### Congratulations! You've implemented a basic text processing pipeline. This will be useful for handling input data in your LLM projects."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1567c1-8a15-4868-a1bf-86bdf1d4c231",
      "metadata": {
        "id": "4d1567c1-8a15-4868-a1bf-86bdf1d4c231"
      },
      "source": [
        "## Extra: Reviewing Tokenization Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad6d4eb1-9c4c-476c-aa53-e7a627d57561",
      "metadata": {
        "id": "ad6d4eb1-9c4c-476c-aa53-e7a627d57561"
      },
      "source": [
        "We'll use `tiktoken`at a later stage for tokenization, so let's see what it does and compare it to another simple tokenization library `NLTK`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0e5db2-4c03-4727-8855-89a294708764",
      "metadata": {
        "id": "be0e5db2-4c03-4727-8855-89a294708764"
      },
      "source": [
        "### Using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ddffd0-0f5c-496e-b782-c27335494a92",
      "metadata": {
        "id": "63ddffd0-0f5c-496e-b782-c27335494a92"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd232433-ce63-4249-a025-2d2b516cc011",
      "metadata": {
        "id": "dd232433-ce63-4249-a025-2d2b516cc011"
      },
      "outputs": [],
      "source": [
        "nltk_tokens = word_tokenize(sample_text)\n",
        "print(\"NLTK Tokens:\", nltk_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c795464-0c53-4f8f-ba79-5c76bf276887",
      "metadata": {
        "id": "9c795464-0c53-4f8f-ba79-5c76bf276887"
      },
      "outputs": [],
      "source": [
        "nltk_code_tokens = word_tokenize(code_text)\n",
        "print(\"NLTK Tokens for Code:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea7ceb9e-0ba1-47cd-a438-5a043d68ab30",
      "metadata": {
        "id": "ea7ceb9e-0ba1-47cd-a438-5a043d68ab30"
      },
      "source": [
        "### Using Tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0a6973-2712-4577-91d6-a2a64fe40342",
      "metadata": {
        "id": "ad0a6973-2712-4577-91d6-a2a64fe40342"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2422a0e7-b5de-4f53-a202-149fcc4cffea",
      "metadata": {
        "id": "2422a0e7-b5de-4f53-a202-149fcc4cffea"
      },
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tiktoken_tokens = enc.encode(sample_text)\n",
        "print(\"Tiktoken Tokens:\", tiktoken_tokens)\n",
        "print(\"Decoded Tiktoken Tokens:\", enc.decode(tiktoken_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b84397a-cee5-4dd5-99fa-d48bb261cb04",
      "metadata": {
        "id": "5b84397a-cee5-4dd5-99fa-d48bb261cb04"
      },
      "outputs": [],
      "source": [
        "print(f\"NLTK token count: {len(nltk_tokens)}\")\n",
        "print(f\"Tiktoken token count: {len(tiktoken_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "056a7701-7cf8-4737-9b04-cb09c080b470",
      "metadata": {
        "id": "056a7701-7cf8-4737-9b04-cb09c080b470"
      },
      "outputs": [],
      "source": [
        "tiktoken_code_tokens = enc.encode(code_text)\n",
        "print(\"\\nTiktoken Tokens (decoded for readability):\")\n",
        "print(enc.decode_tokens_bytes(tiktoken_code_tokens))\n",
        "print(f\"Tiktoken token count: {len(tiktoken_code_tokens)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}